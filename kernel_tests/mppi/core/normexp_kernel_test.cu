#include "normexp_kernel_test.cuh"

template <int NUM_ROLLOUTS>
void launchNormExp_KernelTest(std::array<float, NUM_ROLLOUTS>& trajectory_costs_host, float gamma, float baseline,
                              std::array<float, NUM_ROLLOUTS>& normalized_compute)
{
  // Allocate CUDA memory
  float* trajectory_costs_d;
  HANDLE_ERROR(cudaMalloc((void**)&trajectory_costs_d, sizeof(float) * trajectory_costs_host.size()))

  HANDLE_ERROR(cudaMemcpy(trajectory_costs_d, trajectory_costs_host.data(),
                          sizeof(float) * trajectory_costs_host.size(), cudaMemcpyHostToDevice))

  mppi_common::normExpKernel<<<1, NUM_ROLLOUTS>>>(NUM_ROLLOUTS, trajectory_costs_d, gamma, baseline);
  CudaCheckError();

  HANDLE_ERROR(cudaMemcpy(normalized_compute.data(), trajectory_costs_d, sizeof(float) * trajectory_costs_host.size(),
                          cudaMemcpyDeviceToHost))

  cudaFree(trajectory_costs_d);
}

template <int NUM_ROLLOUTS, int BLOCKSIZE_X>
void launchGenericNormExpKernelTest(std::array<float, NUM_ROLLOUTS> trajectory_costs_host, float gamma, float baseline,
                                    std::array<float, NUM_ROLLOUTS>& normalized_compute)
{
  // Allocate CUDA memory
  float* trajectory_costs_d;
  HANDLE_ERROR(cudaMalloc((void**)&trajectory_costs_d, sizeof(float) * trajectory_costs_host.size()));

  HANDLE_ERROR(cudaMemcpy(trajectory_costs_d, trajectory_costs_host.data(),
                          sizeof(float) * trajectory_costs_host.size(), cudaMemcpyHostToDevice));

  dim3 dimBlock(BLOCKSIZE_X, 1, 1);
  dim3 dimGrid((NUM_ROLLOUTS - 1) / BLOCKSIZE_X + 1, 1, 1);

  mppi_common::normExpKernel<<<dimGrid, dimBlock>>>(NUM_ROLLOUTS, trajectory_costs_d, gamma, baseline);
  CudaCheckError();

  HANDLE_ERROR(cudaMemcpy(normalized_compute.data(), trajectory_costs_d, sizeof(float) * trajectory_costs_host.size(),
                          cudaMemcpyDeviceToHost));

  cudaFree(trajectory_costs_d);
}